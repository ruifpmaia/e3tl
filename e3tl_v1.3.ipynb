{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Spyder Editor\n",
    "\n",
    "This is a temporary script file.\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from IPython.display import Image, display, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(\"img/transform_detail.png\")\n",
    "logfn = \"e3tl_execution_\" + \"{:%d%m%Y_%H%M%S}\".format(datetime.now())\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', filename=logfn + '.txt', level=logging.DEBUG)\n",
    "\n",
    "ALSFRS_Input = 'Tabela_Geral_09_2011_ALS-FRS.csv'\n",
    "ALSFRS_SplitRules_JSON = 'ALSFRS_SplitRules.json'\n",
    "ALSFRS_SemanticRules_JSON = 'ALSFRS_SemanticRules.json'\n",
    "\n",
    "\n",
    "Demographics_Input = 'Tabela_Geral_09_2011_Demographics.csv'\n",
    "Demographics_SplitRules_JSON = 'Demographics_SplitRules.json'\n",
    "Demographics_SemanticRules_JSON = 'Demographics_SemanticRules.json'\n",
    "\n",
    "JoinRules_JSON = 'JoinRules.json' \n",
    "data_dir = '..\\\\..\\\\'\n",
    "\n",
    "fields_columns = ['Input File', 'Output File', 'Input Fields', 'Output Fields', 'Input Rows', 'Output Rows']\n",
    "fields_df = pd.DataFrame(data=None, columns=fields_columns)\n",
    "semantic_columns = ['Input File', 'Output File', 'Input Fields', 'Output Fields', 'Input Rows', 'Output Rows']\n",
    "semantic_df = pd.DataFrame(data=None, columns=semantic_columns)\n",
    "validation_columns = ['Input File', 'Field', 'Validation', 'Ok', 'Error']\n",
    "validation_df = pd.DataFrame(data=None, columns=validation_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a new function:\n",
    "def num_missing(x):\n",
    "    return sum(x.isnull())\n",
    "\n",
    "def missingValues(data):\n",
    "    #Applying per column:\n",
    "    column_mv = data.apply(num_missing, axis=0) #axis=0 defines that function is to be applied on each column\n",
    "    #Applying per row:\n",
    "    row_mv = data.apply(num_missing, axis=1).head() #axis=1 defines that function is to be applied on each row\n",
    "    return row_mv, column_mv\n",
    "\n",
    "\n",
    "def formatDateTime(datestr, outformat):    \n",
    "    DATE_FORMATS = ['%m/%d/%Y %I:%M:%S %p', '%Y/%m/%d %H:%M:%S', '%d/%m/%Y %H:%M', '%m/%d/%Y', '%Y/%m/%d']\n",
    "    my_date = None\n",
    "    error = False\n",
    "    for date_format in DATE_FORMATS:\n",
    "        try:\n",
    "            my_date = datetime.strptime(datestr, date_format).strftime(outformat)\n",
    "            error = False\n",
    "        except ValueError:\n",
    "            error = True\n",
    "            pass\n",
    "        except TypeError:\n",
    "            error = True\n",
    "            break\n",
    "        else:\n",
    "          break\n",
    "    return my_date, error\n",
    "\n",
    "def displayTransparencyReport():\n",
    "    global fields_df\n",
    "    global entries_df\n",
    "    global validation_df\n",
    "    display(HTML('<h1>E3TL Transparency Summary</h1>'))\n",
    "    display(HTML('<h2>Row Split</h2>'))\n",
    "    display(fields_df)\n",
    "    display(HTML('<h2>Semantic Validation</h2>'))\n",
    "    display(semantic_df)\n",
    "    display(HTML('<h2>Data Join</h2>'))\n",
    "    display(validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RowSplit(in_data_fn, in_aux_file):\n",
    "    in_file = data_dir + in_data_fn\n",
    "    in_df = pd.read_csv(in_file, encoding='iso-8859-1', header=0)\n",
    "    logging.info('Input file [%s] shape Rows:%d Columns:%d' % (in_file, in_df.shape[0], in_df.shape[1]))\n",
    "    with open(in_aux_file) as data_file:    \n",
    "        aux_json = json.load(data_file)\n",
    "    exam_features = aux_json['ExamFeatures']\n",
    "    exam_count_per_row = aux_json['ExamCountPerRow']\n",
    "    out_features = aux_json['OutputFeatures']\n",
    "    col_names=[]\n",
    "    for feature in aux_json['FeatureMapping']:\n",
    "        col_names.append(feature['Name'])    \n",
    "    out_df = pd.DataFrame(columns=col_names)\n",
    "    res_list = []\n",
    "    exam_cnt = 0\n",
    "    patient_cnt = 0\n",
    "\n",
    "    validation_errors = {}\n",
    "    validation_errors['Date'] = [0, 0]\n",
    "    \n",
    "    for idx,row in in_df.iterrows():\n",
    "        patient_cnt += 1\n",
    "        for exam in range(exam_count_per_row):\n",
    "            out_row = [0]*out_features\n",
    "            for feature in aux_json['FeatureMapping']:\n",
    "                if (feature['StaticIdx'] == True):\n",
    "                    out_row[feature['OutputIx']] = row[feature['InputIdx']]\n",
    "                else:\n",
    "                    out_row[feature['OutputIx']] = row[(exam*exam_features)+feature['InputIdx']]\n",
    "                if ('Type' in feature and feature['Type'] == 'Date'):\n",
    "                    res, error = formatDateTime(out_row[feature['OutputIx']], feature['Format']) \n",
    "                    out_row[feature['OutputIx']] = res\n",
    "                    if (error):\n",
    "                        validation_errors['Date'][1] = validation_errors['Date'][1] + 1\n",
    "                    else:\n",
    "                        validation_errors['Date'][0] = validation_errors['Date'][0] + 1\n",
    "            exam_cnt += 1\n",
    "            res_list.append(out_row)\n",
    "    out_df = pd.DataFrame(res_list, columns=col_names)\n",
    "    logging.info('Field Mapping output file shape Rows:%d Columns:%d' % (out_df.shape[0], out_df.shape[1]))\n",
    "    out_df.to_csv(aux_json['OutFile'], index=False)\n",
    "    \n",
    "    #report information\n",
    "    global fields_df\n",
    "    fields_df = fields_df.append(pd.DataFrame([[in_file, aux_json['OutFile'], in_df.shape[1], out_df.shape[1], in_df.shape[0], out_df.shape[0]]], columns=fields_columns), ignore_index = True)\n",
    "#   display(fields_df)    \n",
    "    global validation_df    \n",
    "    validation_df = validation_df.append(pd.DataFrame([[in_file, 'Date', 'Format', validation_errors['Date'][0], validation_errors['Date'][1]]], columns=validation_columns), ignore_index = True)\n",
    "    #display(validation_df)   \n",
    "    return aux_json['OutFile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SemanticValidation(in_data_fn, in_aux_file):  \n",
    "    with open(in_aux_file) as data_file:    \n",
    "        aux_json = json.load(data_file)\n",
    "    interm_data = pd.read_csv(in_data_fn, encoding='iso-8859-1', header=0)\n",
    "    logging.info('Appyling rules to intermiary file %s with %d rows and %d columns' % (in_data_fn, interm_data.shape[0], interm_data.shape[1]))    \n",
    "    out_df = pd.DataFrame(data=None, columns=interm_data.columns)\n",
    "    logging.info('For each input file row...apply Validation Rules')\n",
    "    \n",
    "    validation_errors = {}\n",
    "   \n",
    "    for in_idx,row in interm_data.iterrows():\n",
    "        valid_row = True\n",
    "        for rule in aux_json['SemanticRules']:\n",
    "            idx = rule['InputIdx']\n",
    "            field_name = interm_data.columns[idx]\n",
    "            validation_errors[field_name] = validation_errors.get(field_name, {'Threshold':[0,0], 'RegEx':[0,0], 'NotNull':[0,0]})\n",
    "            if ('NotNull' in rule and rule['NotNull'] is True):\n",
    "                v_res = validation_errors[field_name]['NotNull']\n",
    "                if (pd.isnull(row[idx])):\n",
    "                    valid_row = False\n",
    "                    v_res = [v_res[0], v_res[1] + 1]\n",
    "                else:\n",
    "                    v_res = [v_res[0]+1, v_res[1]]\n",
    "                validation_errors[field_name]['NotNull']= v_res\n",
    "            if ('Threshold' in rule):\n",
    "                threshold_sign = rule['Threshold'][0]\n",
    "                threshold_value = float(rule['Threshold'][1:])\n",
    "                v_res = validation_errors[field_name]['Threshold']\n",
    "                try:\n",
    "                    row_value = float(row[idx])\n",
    "                    if (threshold_sign == '<' and row_value >= threshold_value):\n",
    "                        valid_row = False\n",
    "                        v_res = [v_res[0], v_res[1] + 1]\n",
    "                    elif (threshold_sign == '>' and row_value <= threshold_value):                        \n",
    "                        valid_row = False\n",
    "                        v_res = [v_res[0], v_res[1] + 1]\n",
    "                    else:\n",
    "                        v_res = [v_res[0]+1, v_res[1]]\n",
    "                except ValueError:\n",
    "                    valid_row = False\n",
    "                validation_errors[field_name]['Threshold']= v_res\n",
    "            if ('RegEx' in rule):\n",
    "                v_res = validation_errors[field_name]['RegEx']\n",
    "                try:\n",
    "                    test_regex = re.compile(rule['RegEx'])\n",
    "                    if (pd.isnull(row[idx]) or test_regex.match(row[idx]) is None):\n",
    "                        valid_row = False\n",
    "                        v_res = [v_res[0], v_res[1] + 1]\n",
    "                    else:\n",
    "                        v_res = [v_res[0]+1, v_res[1]]\n",
    "                except ValueError:\n",
    "                    valid_row = False\n",
    "                validation_errors[field_name]['RegEx'] = v_res\n",
    "        if (valid_row):\n",
    "            out_df = out_df.append(row)\n",
    "    logging.info('Output format:%d rows x %d columns' % (out_df.shape[0], out_df.shape[1]))\n",
    "    out_df.to_csv(aux_json['OutFile'], index=False)\n",
    "    \n",
    "    #report information\n",
    "    global semantic_df\n",
    "    semantic_df = semantic_df.append(pd.DataFrame([[in_data_fn, aux_json['OutFile'], interm_data.shape[1], out_df.shape[1], interm_data.shape[0], out_df.shape[0]]], columns=semantic_columns), ignore_index = True)\n",
    "    #display(semantic_df)\n",
    "    global validation_df    \n",
    "    temp_validation_df = pd.DataFrame.from_dict(validation_errors)\n",
    "    temp_validation_df['File'] = in_data_fn\n",
    "    #display(temp_validation_df)\n",
    "    #validation_df = validation_df.append(temp_validation_df, ignore_index = True)\n",
    "    #display(validation_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DataJoin(rules):\n",
    "    with open(rules) as data_file:    \n",
    "        aux_json = json.load(data_file)\n",
    "#    interm_data = pd.read_csv(intermediate_fn, encoding='iso-8859-1', header=0)\n",
    "#    print ('Appyling rules to intermiary file %s with %d rows and %d columns' % (intermediate_fn, interm_data.shape[0], interm_data.shape[1]))    \n",
    "    out_df = pd.DataFrame(data=None)    \n",
    "    keys = []\n",
    "    in_dfs = {}\n",
    "    for rule in aux_json['FeatureMapping']:\n",
    "        logging.info (rule)\n",
    "        if (rule[\"InputFile\"] not in in_dfs):\n",
    "            logging.info('Loading DataFrame from:' + rule[\"InputFile\"])\n",
    "            in_dfs[rule[\"InputFile\"]] = pd.read_csv(rule[\"InputFile\"], encoding='iso-8859-1', header=0)    \n",
    "        if ('JoinKey' in rule and rule['JoinKey'] is True):\n",
    "            keys.extend([rule['Name']])\n",
    "    logging.info('Columns to be used as Keys:' + str(keys))\n",
    "    in_dfs = [ v for v in in_dfs.values() ]\n",
    "    out_df = in_dfs[0].join(in_dfs[1], on=keys, how='inner', rsuffix='r_')\n",
    "    out_df.to_csv(aux_json['OutFile'], index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1:m Row Split</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intermediary_data_fn = RowSplit(ALSFRS_Input, ALSFRS_SplitRules_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intermediary_data_fn2 = RowSplit(Demographics_Input, Demographics_SplitRules_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Semantic Validation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SemanticValidation(intermediary_data_fn, ALSFRS_SemanticRules_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SemanticValidation(intermediary_data_fn2, Demographics_SemanticRules_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Join</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataJoin(JoinRules_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>E3TL Transparency Summary</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Row Split</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input File</th>\n",
       "      <th>Output File</th>\n",
       "      <th>Input Fields</th>\n",
       "      <th>Output Fields</th>\n",
       "      <th>Input Rows</th>\n",
       "      <th>Output Rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\..\\Tabela_Geral_09_2011_ALS-FRS.csv</td>\n",
       "      <td>ALSFRS_f.csv</td>\n",
       "      <td>243.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>5522.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\..\\Tabela_Geral_09_2011_Demographics.csv</td>\n",
       "      <td>Demographics_f.csv</td>\n",
       "      <td>71.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>616.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Input File         Output File  \\\n",
       "0       ..\\..\\Tabela_Geral_09_2011_ALS-FRS.csv        ALSFRS_f.csv   \n",
       "1  ..\\..\\Tabela_Geral_09_2011_Demographics.csv  Demographics_f.csv   \n",
       "\n",
       "   Input Fields  Output Fields  Input Rows  Output Rows  \n",
       "0         243.0            6.0       502.0       5522.0  \n",
       "1          71.0           13.0       616.0        616.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Semantic Validation</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input File</th>\n",
       "      <th>Output File</th>\n",
       "      <th>Input Fields</th>\n",
       "      <th>Output Fields</th>\n",
       "      <th>Input Rows</th>\n",
       "      <th>Output Rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALSFRS_f.csv</td>\n",
       "      <td>ALSFRS_valid.csv</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5522.0</td>\n",
       "      <td>1151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Demographics_f.csv</td>\n",
       "      <td>Demographics_valid.csv</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>356.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Input File             Output File  Input Fields  Output Fields  \\\n",
       "0        ALSFRS_f.csv        ALSFRS_valid.csv           6.0            6.0   \n",
       "1  Demographics_f.csv  Demographics_valid.csv          13.0           13.0   \n",
       "\n",
       "   Input Rows  Output Rows  \n",
       "0      5522.0       1151.0  \n",
       "1       616.0        356.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Data Join</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input File</th>\n",
       "      <th>Field</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Ok</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\..\\Tabela_Geral_09_2011_ALS-FRS.csv</td>\n",
       "      <td>Date</td>\n",
       "      <td>Format</td>\n",
       "      <td>1392.0</td>\n",
       "      <td>4130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\..\\Tabela_Geral_09_2011_Demographics.csv</td>\n",
       "      <td>Date</td>\n",
       "      <td>Format</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Input File Field Validation      Ok  \\\n",
       "0       ..\\..\\Tabela_Geral_09_2011_ALS-FRS.csv  Date     Format  1392.0   \n",
       "1  ..\\..\\Tabela_Geral_09_2011_Demographics.csv  Date     Format     0.0   \n",
       "\n",
       "    Error  \n",
       "0  4130.0  \n",
       "1     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayTransparencyReport()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
